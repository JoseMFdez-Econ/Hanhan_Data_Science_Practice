{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import operator\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Anomalies Detection\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sparkCt = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(lst, indices, unique_values, c):\n",
    "    zs = [0.0]*c\n",
    "    rest_lst = [float(lst[k]) for k in range(len(lst)) if k not in indices]\n",
    "    for pos in indices:\n",
    "        idx = unique_values.index(Row(lst[pos]))\n",
    "        zs[idx] = 1.0\n",
    "    zs.extend(rest_lst)\n",
    "    return zs\n",
    "\n",
    "class AnomalyDetection():\n",
    "\n",
    "    def readData(self, filename):\n",
    "        self.rawDF = spark.read.parquet(filename).cache()\n",
    "\n",
    "\n",
    "    def cat2Num(self, df, indices):\n",
    "        unique_values = []\n",
    "        for i in indices:\n",
    "            d = udf(lambda r: r[i], StringType())\n",
    "            dt = df.select(d(df.rawFeatures)).distinct().collect()\n",
    "            unique_values.extend(dt)\n",
    "\n",
    "        unique_count = len(unique_values)\n",
    "        convertUDF = udf(lambda r: to_onehot(r, indices, unique_values, unique_count), ArrayType(DoubleType()))\n",
    "        newdf = df.withColumn(\"features\", convertUDF(df.rawFeatures))\n",
    "\n",
    "        return newdf\n",
    "\n",
    "\n",
    "    def addScore(self, df):\n",
    "        cluster_dict = {}\n",
    "        clusters_list = df.select(\"prediction\").collect()\n",
    "        for c in clusters_list:\n",
    "            cluster_dict[c] = cluster_dict.setdefault(c,0.0)+1.0\n",
    "        sorted_clusters = sorted(cluster_dict.items(), key=operator.itemgetter(1))  # sort by value\n",
    "        n_max = sorted_clusters[-1][1]\n",
    "        n_min = sorted_clusters[0][1]\n",
    "        score_udf = udf(lambda p: float(n_max - cluster_dict.get(Row(p)))/(n_max - n_min), DoubleType())\n",
    "        score_df = df.withColumn(\"score\", score_udf(df.prediction))\n",
    "        return score_df\n",
    "\n",
    "\n",
    "    def detect(self, k, t):\n",
    "        # Encoding categorical features using one-hot.\n",
    "        df1 = self.cat2Num(self.rawDF, [0, 1]).cache()\n",
    "\n",
    "        # Clustering points using KMeans\n",
    "        features = df1.select(\"features\").rdd.map(lambda row: row[0]).cache()\n",
    "        model = KMeans.train(features, k, maxIterations=40, initializationMode=\"random\", seed=20)\n",
    "\n",
    "        # Adding the prediction column to df1\n",
    "        modelBC = sparkCt.broadcast(model)\n",
    "        predictUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n",
    "        df2 = df1.withColumn(\"prediction\", predictUDF(df1.features)).cache()\n",
    "\n",
    "        # Adding the score column to df2; The higher the score, the more likely it is an anomaly\n",
    "        df3 = self.addScore(df2).cache()\n",
    "\n",
    "        return df3.where(df3.score > t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|   id|         rawFeatures|            features|prediction|score|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|44362|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|44432|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|44776|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|44928|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|45317|[tcp, SF, -0.1578...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|45323|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|45669|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|45676|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|45986|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|46341|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|46376|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|46391|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|46547|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|46872|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|47450|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|47951|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|48194|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|48281|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|48569|[tcp, SF, 0.47745...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "|48883|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         7|  1.0|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "\n",
    "def main():\n",
    "    ad = AnomalyDetection()\n",
    "    inputs = \"data/logs-features-sample/\"\n",
    "    ad.readData(inputs)\n",
    "    anomalies = ad.detect(8, 0.97)\n",
    "    \n",
    "    log_param(\"output_records\", anomalies.count())\n",
    "    log_param(\"output_columns\", len(anomalies.columns))\n",
    "    \n",
    "    anomalies.show()\n",
    "    \n",
    "    with open(\"mlflow_output.txt\", \"w\") as f:\n",
    "        f.write(\"Finish running!\")\n",
    "    log_artifact(\"mlflow_output.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES\n",
    "\n",
    "* param, metrics will all be written in to folder \"mlruns\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
