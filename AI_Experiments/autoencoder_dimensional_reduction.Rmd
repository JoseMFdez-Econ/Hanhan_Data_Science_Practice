---
title: "Dimensional Reduction - AutoEncoder vs PCA"
output:
  html_notebook: default
  pdf_document: default
---

### Load Data
```{r}
# devtools::install_github("ropensci/plotly")
library(plotly)
library(DAAG)
library(ggplot2)
head(ais)
```


### Normalize Data
```{r}
min_max <- function(x) {
  return ((x - min(x))/(max(x) - min(x)))
}

x_train <- apply(ais[, 1:11], 2, min_max)

head(x_train)
```

### PCA for Dimensional Reduction
```{r}
pca <- prcomp(x_train)

pca
```

```{r}
qplot(x = 1:11, y = cumsum(pca$sdev)/sum(pca$sdev), geom="line")
```
As shown above, the first 6 components can explain 90% variance of the data.

Below shows how the first 2 principle components seperates male and female:
```{r}
ggplot(data.frame(PC1=as.data.frame(pca$x)$PC1, PC2=as.data.frame(pca$x)$PC2), 
       aes(x = PC1, y = PC2, col = ais$sex)) + geom_point()
```



Below shows how the first 3 principle components seperates male and female:
```{r}
pca_plotly <- plot_ly(as.data.frame(pca$x), x = ~PC1, y = ~PC2, z = ~PC3, color = ~ais$sex) %>% add_markers()

pca_plotly
```


### AutoEncoder for Dimensional Reduction
```{r}
# devtools::install_github("rstudio/tensorflow")  # make sure you have installed devtools
library("tensorflow")
library("keras")
```

```{r}
# I have already installed keras with tensorflow backend through Conda, so no need to install again here
reticulate::py_discover_config()  # this will show available installed tensorflow in python
keras::is_keras_available()  # If this reurns True, it means you Keras with tensorflow is available
```

```{r}
head(x_train)
```

```{r}
x_train <- as.matrix(x_train)

head(x_train)
```

#### Try 2 dimensions in the encoded layer
```{r}
model <- keras_model_sequential()

model %>%
  layer_dense(units=6, activation="tanh",
              input_shape = ncol(x_train)) %>%
  layer_dense(units=2, activation="tanh", name="bottleneck") %>%
  layer_dense(units=6, activation="tanh") %>%
  layer_dense(units=ncol(x_train))
```


```{r}
summary(model)
```

```{r}
model %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam"
)
```

```{r}
model %>% fit(
  x = x_train,
  y = x_train,  # here y is still x_train
  epochs = 2000,
  verbose = 0
)
```

```{r}
mse.ae2 <- evaluate(model, x_train, x_train)
mse.ae2
```

```{r}
# extract the encoded layer (bottleneck layer)
intermediate_layer_model <- keras_model(inputs = model$input,
                                   outputs = get_layer(model, "bottleneck")$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
```

Below shows how the encoded layer seperates male and female:
To be honest, I didn't see much difference here, comparing with PCA method above....
```{r}
ggplot(data.frame(PC1 = intermediate_output[,1], PC2 = intermediate_output[,2]), 
       aes(x = PC1, y = PC2, col = ais$sex)) + geom_point()
```

#### Try 3 dimensions in the encoded layer
```{r}
model2 <- keras_model_sequential()

model2 %>%
  layer_dense(units=6, activation="tanh",
              input_shape = ncol(x_train)) %>%
  layer_dense(units=3, activation="tanh", name="bottleneck") %>%
  layer_dense(units=6, activation="tanh") %>%
  layer_dense(units=ncol(x_train))
```

```{r}
summary(model2)
```

```{r}
# compile model
model2 %>% compile(
  loss = "mean_squared_error", 
  optimizer = "adam"
)

# fit model
model2 %>% fit(
  x = x_train, 
  y = x_train, 
  epochs = 2000,
  verbose = 0
)

# evaluate the model
evaluate(model2, x_train, x_train)
```

It seems that it does seperate better than PCA above
```{r}
intermediate_layer_model <- keras_model(inputs = model2$input, outputs = get_layer(model2, "bottleneck")$output)
intermediate_output <- predict(intermediate_layer_model, x_train)

# plot the reduced dat set
aedf <- data.frame(node1 = intermediate_output[,1], node2 = intermediate_output[,2], node3 = intermediate_output[,3])
ae_plotly <- plot_ly(aedf, x = ~node1, y = ~node2, z = ~node3, color = ~ais$sex) %>% add_markers()

ae_plotly
```

## Reconstruction Error: PCA vs AutoEncoder
```{r}
# pCA reconstruction
pca.recon <- function(pca, x, k){
  mu <- matrix(rep(pca$center, nrow(pca$x)), nrow = nrow(pca$x), byrow = T)
  recon <- pca$x[,1:k] %*% t(pca$rotation[,1:k]) + mu
  mse <- mean((recon - x)^2)
  return(list(x = recon, mse = mse))
}

xhat <- rep(NA, 10)
for(k in 1:10){
  xhat[k] <- pca.recon(pca, x_train, k)$mse
}
```


```{r}
# Autoencoder reconstruction
# autoencoder last layer is reconstruction
ae.mse <- rep(NA, 5)
for(k in 1:5){
  modelk <- keras_model_sequential()
  modelk %>%
    layer_dense(units = 6, activation = "tanh", input_shape = ncol(x_train)) %>%
    layer_dense(units = k, activation = "tanh", name = "bottleneck") %>%
    layer_dense(units = 6, activation = "tanh") %>%
    layer_dense(units = ncol(x_train))

  modelk %>% compile(
    loss = "mean_squared_error", 
    optimizer = "adam"
  )

  modelk %>% fit(
    x = x_train, 
    y = x_train, 
    epochs = 5000,
    verbose = 0
  )

  ae.mse[k] <- unname(evaluate(modelk, x_train, x_train))
}
```

```{r}
df <- data.frame(k = c(1:10, 1:5), mse = c(xhat, ae.mse), method = c(rep("pca", 10), rep("autoencoder", 5)))
ggplot(df, aes(x = k, y = mse, col = method)) + geom_line()
```
#### Observation
* When k is small, autoencoder has less reconstruction error
* When k is larger, autoencoder and PCA apepar to have almost the same reconstruction error


